import os
import re
import warnings
from typing import List, Dict

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PDFPlumberLoader,PyMuPDFLoader
from langchain_teddynote.document_loaders import HWPLoader
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.tools import Tool
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_community.tools.tavily_search.tool import TavilySearchResults
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerank

warnings.filterwarnings("ignore")

# === ê¸°ë³¸ ì„¤ì • ===
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
embeddings = OpenAIEmbeddings()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)

# === í•„í„° ì¶”ì¶œ í•¨ìˆ˜ ===
def extract_filter_from_query(query: str) -> Dict:
    crop_match = re.search(r"(ë”¸ê¸°|í† ë§ˆí† |ë§ê³ )", query)
    pest_match = re.search(r"([ê°€-í£]+ë³‘|ì‘ì• |ì§„ë”§ë¬¼)", query)
    fertilizer_match = re.search(r"(ë¹„ë£Œ|í‡´ë¹„|ì§ˆì†Œ|ì¸ì‚°|ì¹¼ë¥¨)", query)
    cultivation_match = re.search(r"(ì¬ë°°|íŒŒì¢…|ì •ì‹|ê´€ë¦¬|ê´€ìˆ˜|ì‹œë¹„)", query)

    filters = {}
    if crop_match:
        filters["ì‘ë¬¼"] = crop_match.group(1)

    if pest_match:
        filters["ë³‘í•´ì¶©ëª…"] = pest_match.group(1)

    if fertilizer_match:
        filters["ì§ˆë¬¸ìœ í˜•"] = "ë¹„ë£Œ"
    elif cultivation_match:
        filters["ì§ˆë¬¸ìœ í˜•"] = "ì¬ë°°"
    

    return filters

def infer_task_type(text: str) -> str:
    if re.search(r"(ë³‘|í•´ì¶©|ì‘ì• |ê³°íŒ¡ì´|ì§„ë”§ë¬¼)", text):
        return "ë³‘í•´ì¶©"
    elif re.search(r"(ì§ˆì†Œ|ì¸ì‚°|ì¹¼ë¥¨|í‡´ë¹„|ë¹„ë£Œ)", text):
        return "ë¹„ë£Œ"
    elif re.search(r"(ì •ì‹|íŒŒì¢…|ê´€ìˆ˜|ì‹œë¹„|ìˆ˜í™•|ì¬ë°°|ìœ ì¸|ì˜¨ë„|í† ì–‘)", text):
        return "ì¬ë°°"
    else:
        return "ì¼ë°˜"

# === ë¬¸ì„œ ë¡œë” ===
def load_docs_with_metadata(paths: List[str], crop: str) -> List:
    all_docs = []

    for path in paths:
        if path.lower().endswith(".pdf"):
            loader = PyMuPDFLoader(path)
        elif path.lower().endswith(".hwp"):
            loader = HWPLoader(path)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {path}")

        docs = loader.load()
        for i, doc in enumerate(docs):
            doc.metadata.update({
                "ì‘ë¬¼": crop,
                "ì¶œì²˜": os.path.basename(path),
                "í˜ì´ì§€": i
            })

            task_type = infer_task_type(doc.page_content)
            doc.metadata["ì§ˆë¬¸ìœ í˜•"] = task_type

            if task_type == "ë³‘í•´ì¶©":
                match = re.search(r"\d+\.\s*([ê°€-í£]+ë³‘|ì‘ì• |ì§„ë”§ë¬¼)", doc.page_content)
                if match:
                    doc.metadata["ë³‘í•´ì¶©ëª…"] = ",".join(sorted(set(match)))

            if task_type == "ë¹„ë£Œ":
                fert_matches = re.findall(r"(ì§ˆì†Œ|ì¸ì‚°|ì¹¼ë¥¨|í‡´ë¹„|ë¹„ë£Œ)", doc.page_content)
                if fert_matches:
                    doc.metadata["ë¹„ë£Œì„±ë¶„"] = ",".join(sorted(set(fert_matches)))

            if task_type == "ì¬ë°°":
                cultivation_matches = re.findall(r"(ì •ì‹|íŒŒì¢…|ê´€ìˆ˜|ì‹œë¹„|ìˆ˜í™•|ìœ ì¸|ì˜¨ë„|í† ì–‘)", doc.page_content)
                if cultivation_matches:
                    doc.metadata["ì¬ë°°í•­ëª©"] = ",".join(sorted(set(cultivation_matches)))

        all_docs.extend(docs)

    return all_docs


# 3. ì›¹ ê²€ìƒ‰ íˆ´
def web_search_with_notice(query: str) -> str:
    results = TavilySearchResults().invoke(query)
    return str(results) + "\n\nğŸ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì œê³µëœ ì •ë³´ì…ë‹ˆë‹¤."

web_search_tool = Tool(
    name="web_search_tool",
    func=web_search_with_notice,
    description=(
       "ë”¸ê¸°, í† ë§ˆí† , ë§ê³ ì— ëŒ€í•œ ì‘ë¬¼ ì¬ë°°ë²•, ë³‘í•´ì¶©, ë†ì•½ ë° ë¹„ë£Œ(í‡´ë¹„) ê´€ë ¨ ì •ë³´ê°€ ë¬¸ì„œì— ì—†ê±°ë‚˜ ë¶€ì¡±í•  ê²½ìš°, ì›¹ì—ì„œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. "
    "ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°ì—ë„ ì‚¬ìš©í•˜ì„¸ìš”. "
    "ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•œ ì‘ë‹µì¼ ê²½ìš°, ë°˜ë“œì‹œ 'ğŸ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì œê³µëœ ì •ë³´ì…ë‹ˆë‹¤.' ë¬¸êµ¬ë¥¼ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤."
    )
)

# 4. Chroma ë²¡í„° DB ì²˜ë¦¬
def get_vectorstore(pdf_path, index_path, crop):
    if os.path.exists(index_path):
        return Chroma(persist_directory=index_path, embedding_function=embeddings)
    docs = load_docs_with_metadata(pdf_path, crop)
    split_docs = text_splitter.split_documents(docs)
    vs = Chroma.from_documents(split_docs, embeddings, persist_directory=index_path)
    vs.persist()
    return vs

# ê²½ë¡œ
strawberry_path = ["data/ë”¸ê¸° ë³‘í•´ì¶© ë° ë¹„ë£Œ.pdf",
                   "data/ë”¸ê¸°(ì´‰ì„±ì¬ë°°) ë†ì‘ì—…ì¼ì •.hwp",
                   "data/ê³¼ì¦™ì´í’ë¶€í•œì™•ë”¸ê¸°í‚¹ìŠ¤ë² ë¦¬ì¬ë°°ë§¤ë‰´ì–¼.pdf"]
tomato_path = ["data/í† ë§ˆí†  ë³‘í•´ì¶© ë° ë¹„ë£Œ.pdf",
               "data/í† ë§ˆí† ,ë°©ìš¸í† ë§ˆí†  ë†ì‘ì—…ì¼ì •.hwp",
               "data/ê³¼ì±„ë¥˜(í† ë§ˆí† ).pdf",
               "data/29 í† ë§ˆí† _ì €í™”ì§ˆ_ë‹¨ë©´.pdf"]
mango_path = ["data/ì•„ì—´ëŒ€ê³¼ìˆ˜(ë§ê³ ).pdf",
              "data/2023ë†ì—…ê¸°ìˆ ê¸¸ì¡ì´_ë§ê³ _ë‹¨ë©´.pdf"]

strawberry_index_path = "chroma_index/strawberry"
tomato_index_path = "chroma_index/tomato"
mango_index_path = "chroma_index/mango"

strawberry_vectorstore = get_vectorstore(strawberry_path, strawberry_index_path, "ë”¸ê¸°")
tomato_vectorstore = get_vectorstore(tomato_path, tomato_index_path, "í† ë§ˆí† ")
mango_vectorstore = get_vectorstore(mango_path, mango_index_path, "ë§ê³ ")

# 5. í•„í„° ê¸°ë°˜ retriever tool ë˜í•‘
def make_filtered_tool(name, vectorstore, crop_name):
    # 1. ê¸°ë³¸ Retriever
    base_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})  # í›„ë³´ ë¬¸ì„œ ì¶©ë¶„íˆ í™•ë³´

    # 2. Cohere Reranker ì„¤ì •
    reranker = CohereRerank(
    cohere_api_key=os.getenv("COHERE_API_KEY"),
    top_n=5,
    model="rerank-multilingual-v3.0"  # âœ… í•œêµ­ì–´ ì§€ì› ëª¨ë¸
)

    # 3. Rerankerë¡œ ì••ì¶•ëœ Retriever êµ¬ì„±
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=reranker,
        base_retriever=base_retriever
    )

    # 4. í•„í„° ê¸°ë°˜ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
    def _filtered_search(query: str) -> str:
        filters = extract_filter_from_query(query)
        return compression_retriever.invoke(query, filter=filters)  # í•„í„° ì ìš©

    return Tool(
        name=name,
        func=_filtered_search,
        description=(
            f"{crop_name}ì— ëŒ€í•œ ë³‘í•´ì¶© ì§„ë‹¨, íš¨ê³¼ì ì¸ ë†ì•½ ë° ë¹„ë£Œ(í‡´ë¹„) ì¶”ì²œ, ê·¸ë¦¬ê³  ì‘ë¬¼ ìƒìœ¡ ë‹¨ê³„ë³„ ì¬ë°° ë°©ë²• ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. "
            "ì§ˆë¬¸ì— ë”°ë¼ ë³‘í•´ì¶©ëª…, ë†ì•½ ì‚¬ìš© ì‹œê¸°ì™€ ë°©ë²•, ë¹„ë£Œ ì„±ë¶„ê³¼ ìš©ëŸ‰, íŒŒì¢…ë¶€í„° ìˆ˜í™•ê¹Œì§€ì˜ ì¬ë°° ì¼ì • ë° ê´€ë¦¬ ë°©ë²•ì„ í¬í•¨í•´ ì•ˆë‚´í•©ë‹ˆë‹¤."
        )
    )

strawberry_retriever_tool = make_filtered_tool("strawberry", strawberry_vectorstore, "ë”¸ê¸°")
tomato_retriever_tool = make_filtered_tool("tomato", tomato_vectorstore, "í† ë§ˆí† ")
mango_retriever_tool = make_filtered_tool("mango", mango_vectorstore, "ë§ê³ ")

tools = [strawberry_retriever_tool, tomato_retriever_tool, mango_retriever_tool, web_search_tool]

prompt = ChatPromptTemplate.from_messages([
    ("system",
"""
ë‹¹ì‹ ì€ ì‘ë¬¼ ì¬ë°°, ë³‘í•´ì¶© ì§„ë‹¨, ë†ì•½ ë° ë¹„ë£Œ(í‡´ë¹„) ì¶”ì²œê¹Œì§€ ì „ë°˜ì ì¸ ë†ì—… ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ì•„ë˜ ê·œì¹™ì— ë”°ë¼ ê´€ë ¨ ì •ë³´ë¥¼ ì •í™•í•˜ê³  ì‹¤ìš©ì ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

---

â“ ì§ˆë¬¸ ë²”ì£¼ ë° ì¶œë ¥ í˜•ì‹:

ğŸ“Œ ë³‘í•´ì¶©ì— ëŒ€í•œ ì§ˆë¬¸ì¼ ê²½ìš°:
âœ… ë³‘í•´ì¶© ì´ë¦„  
ğŸ§´ ì¶”ì²œ ë†ì•½  
ğŸ’Š ë†ì•½ ì‚¬ìš© ë°©ë²•  
ğŸ•“ ë†ì•½ ì‚¬ìš© ì‹œê¸°  
ğŸ§° ë°©ì œ ë°©ë²•  

ğŸ“Œ ë†ì•½ì— ëŒ€í•œ ì§ˆë¬¸ì¼ ê²½ìš°:
ğŸ§´ ì¶”ì²œ ë†ì•½  
ğŸ’Š ë†ì•½ ì‚¬ìš© ë°©ë²•  
ğŸ•“ ë†ì•½ ì‚¬ìš© ì‹œê¸°  

ğŸ“Œ ë¹„ë£Œë‚˜ í‡´ë¹„ì— ëŒ€í•œ ì§ˆë¬¸ì¼ ê²½ìš°:
ğŸŒ¿ í•„ìš”í•œ ì˜ì–‘ì†Œ  
ğŸ“† ë¹„ë£Œ ì‚¬ìš© ì‹œê¸°  
ğŸ§ª ë¹„ë£Œ ì‚¬ìš© ë°©ë²•  
âš–ï¸ ë¹„ë£Œ ì‚¬ìš© ìš©ëŸ‰  

ğŸ“Œ ì‘ë¬¼ ì¬ë°°ë²•ì´ ê¶ê¸ˆí•œ ê²½ìš°:
ğŸŒ± ì¬ë°° ì‹œê¸°  
ğŸŒ ì¬ë°° í™˜ê²½ ì¡°ê±´ (ê¸°í›„, í† ì–‘ ë“±)  
ğŸ“ ì¬ì‹ ê°„ê²© ë° ì •ì‹ ë°©ë²•  
ğŸ’§ ê´€ìˆ˜ ë°©ë²•  
ğŸ§ª ì‹œë¹„(ë¹„ë£Œ) ë°©ë²•  
âœ‚ï¸ ìƒìœ¡ ê´€ë¦¬ (ê°€ì§€ì¹˜ê¸°, ìœ ì¸ ë“±)  
ğŸŒ¾ ìˆ˜í™• ì‹œê¸° ë° ë°©ë²•  

---

ğŸ› ï¸ ë„êµ¬ ì‚¬ìš© ì§€ì¹¨:

- ê´€ë ¨ ì •ë³´ëŠ” crop_retriever_toolì„ í†µí•´ ê²€ìƒ‰í•˜ì„¸ìš”. (ex: strawberry_retriever_tool, mango_retriever_tool ë“±)
- ë¬¸ì„œì— ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° web_search_toolì„ í†µí•´ ë³´ì™„í•˜ì„¸ìš”.
- ì›¹ ê²€ìƒ‰ íˆ´ ì‚¬ìš© ì‹œ, ê²°ê³¼ í•˜ë‹¨ì— ë°˜ë“œì‹œ ì•„ë˜ ë¬¸êµ¬ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:  
  ğŸ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì œê³µëœ ì •ë³´ì…ë‹ˆë‹¤.

ğŸ’¡ ì´ˆë³´ ë†ì—…ì¸ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ê°„ê²°í•˜ê³  ì‰¬ìš´ ì–¸ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

ğŸ“š ì°¸ê³ :  
ìƒì„¸ ì‚¬ìš©ë²•ì€ ë†ì•½Â·ë¹„ë£Œ ë¼ë²¨ ë˜ëŠ” ë†ì´Œì§„í¥ì²­ ìë£Œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
"""
    ),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])


# Agent ì‹¤í–‰ê¸° êµ¬ì„±
agent = create_tool_calling_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)

# ì˜ˆì‹œ ì‹¤í–‰
response = agent_executor.invoke({
    "input": "ë§ê³  ì¬ë°°ë°©ë²•ì„ ì•Œë ¤ì¤˜"
})

print(response["output"].strip())

